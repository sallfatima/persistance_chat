"""
Frontend Chainlit pour Solution 1 (Redis + PostgreSQL)
Affiche si rÃ©ponse vient du cache ou non
"""

import chainlit as cl
import httpx
import asyncio
import os
from typing import Optional
from datetime import datetime

# ==================== CONFIGURATION ====================

BACKEND_URL = os.getenv("BACKEND_URL", "http://localhost:8000")

# ModÃ¨les disponibles
OPENAI_MODELS = {
    "gpt-4o": "GPT-4o (RecommandÃ©)",
    "gpt-4o-mini": "GPT-4o Mini (Ã‰conomique)",
    "gpt-4-turbo-preview": "GPT-4 Turbo",
    "gpt-3.5-turbo": "GPT-3.5 Turbo"
}

ANTHROPIC_MODELS = {
    "claude-3-5-sonnet-20241022": "Claude 3.5 Sonnet (RecommandÃ©)",
    "claude-3-opus-20240229": "Claude 3 Opus",
    "claude-3-haiku-20240307": "Claude 3 Haiku (Ã‰conomique)"
}

# ==================== HELPERS ====================

async def check_backend_health() -> dict:
    """VÃ©rifier santÃ© du backend"""
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(f"{BACKEND_URL}/health", timeout=5.0)
            return response.json()
    except Exception as e:
        return {"status": "error", "error": str(e)}

async def get_backend_stats() -> dict:
    """RÃ©cupÃ©rer statistiques backend"""
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(f"{BACKEND_URL}/api/stats", timeout=5.0)
            return response.json()
    except Exception as e:
        return {}

async def create_task(prompt: str, provider: str, model: str, temperature: float) -> dict:
    """CrÃ©er une tÃ¢che de gÃ©nÃ©ration sur le backend"""
    async with httpx.AsyncClient() as client:
        response = await client.post(
            f"{BACKEND_URL}/api/chat/generate",
            json={
                "prompt": prompt,
                "provider": provider,
                "model": model,
                "temperature": temperature
            },
            timeout=10.0
        )
        return response.json()

async def stream_from_backend(task_id: str, msg: cl.Message, cached: bool):
    """Stream depuis backend avec polling"""
    
    # Si cached, afficher badge immÃ©diatement
    if cached:
        await msg.stream_token("ğŸ¯ **CACHE HIT** - RÃ©ponse instantanÃ©e depuis Redis !\n\n")
        await msg.stream_token("---\n\n")
    
    last_chunk_id = 0
    start_time = datetime.now()
    
    while True:
        # RÃ©cupÃ©rer nouveaux chunks
        async with httpx.AsyncClient() as client:
            response = await client.get(
                f"{BACKEND_URL}/api/chat/chunks/{task_id}?from_id={last_chunk_id}",
                timeout=5.0
            )
            data = response.json()
        
        # Afficher chunks
        for chunk in data["chunks"]:
            await msg.stream_token(chunk["text"])
            last_chunk_id = chunk["chunk_id"] + 1
        
        # Check status
        async with httpx.AsyncClient() as client:
            status_response = await client.get(
                f"{BACKEND_URL}/api/chat/status/{task_id}",
                timeout=5.0
            )
            status = status_response.json()
        
        if status.get("status") == "completed":
            # Calculer temps
            elapsed = (datetime.now() - start_time).total_seconds()
            
            # Afficher footer
            await msg.stream_token("\n\n---\n\n")
            await msg.stream_token(f"â±ï¸ Temps: {elapsed:.2f}s | ")
            await msg.stream_token(f"Provider: {status['provider']} | ")
            await msg.stream_token(f"Model: {status['model']}\n")
            
            if cached:
                await msg.stream_token("ğŸ’¾ RÃ©ponse servie depuis le cache Redis (TTL: 1h)\n")
            else:
                await msg.stream_token("ğŸ†• Nouvelle gÃ©nÃ©ration - Maintenant en cache pour 1h\n")
            
            break
        
        if status.get("status") == "error":
            await msg.stream_token(f"\n\nâŒ Error: {status.get('error')}")
            break
        
        await asyncio.sleep(0.2)

# ==================== CHAINLIT HANDLERS ====================

@cl.on_chat_start
async def start():
    """Initialisation de la conversation"""
    
    # VÃ©rifier backend
    health = await check_backend_health()
    
    if health.get("status") == "error":
        await cl.Message(
            content=f"âŒ **Backend inaccessible**\n\n"
                    f"Erreur: {health.get('error')}\n\n"
                    f"Assurez-vous que le backend tourne sur {BACKEND_URL}"
        ).send()
        return
    
    # RÃ©cupÃ©rer stats
    stats = await get_backend_stats()
    
    # CrÃ©er settings
    settings = await cl.ChatSettings(
        [
            cl.input_widget.Select(
                id="Provider",
                label="ğŸ¤– LLM Provider",
                values=["OpenAI GPT", "Anthropic Claude"],
                initial_value="OpenAI GPT"
            ),
            cl.input_widget.Select(
                id="Model",
                label="ğŸ“¦ ModÃ¨le",
                values=list(OPENAI_MODELS.keys()),
                initial_value="gpt-4o"
            ),
            cl.input_widget.Slider(
                id="Temperature",
                label="ğŸŒ¡ï¸ TempÃ©rature",
                initial=0.7,
                min=0.0,
                max=2.0,
                step=0.1
            )
        ]
    ).send()
    
    # Init session
    cl.user_session.set("provider", "openai")
    cl.user_session.set("model", "gpt-4o")
    cl.user_session.set("temperature", 0.7)
    
    # Message de bienvenue
    welcome_msg = "# ğŸš€ Chatbot LLM - Redis + PostgreSQL\n\n"
    welcome_msg += "## Architecture\n\n"
    welcome_msg += "```\n"
    welcome_msg += "Chainlit â†’ FastAPI â†’ Redis (cache) + PostgreSQL (data) â†’ GPT/Claude\n"
    welcome_msg += "```\n\n"
    
    welcome_msg += "## Backend Status\n\n"
    welcome_msg += f"- OpenAI: {'ğŸŸ¢ ConfigurÃ©' if health.get('openai_configured') else 'ğŸ”´ Non configurÃ©'}\n"
    welcome_msg += f"- Anthropic: {'ğŸŸ¢ ConfigurÃ©' if health.get('anthropic_configured') else 'ğŸ”´ Non configurÃ©'}\n"
    welcome_msg += f"- Redis: {'ğŸŸ¢ ConnectÃ©' if health.get('redis_connected') else 'ğŸ”´ DÃ©connectÃ©'}\n"
    welcome_msg += f"- PostgreSQL: {'ğŸŸ¢ ConnectÃ©' if health.get('postgres_connected') else 'ğŸ”´ DÃ©connectÃ©'}\n\n"
    
    if stats:
        welcome_msg += "## Statistiques Cache\n\n"
        welcome_msg += f"- Total gÃ©nÃ©rations: {stats.get('total_tasks', 0)}\n"
        welcome_msg += f"- Depuis cache: {stats.get('cached_tasks', 0)}\n"
        welcome_msg += f"- Taux de hit: {stats.get('cache_hit_rate', '0%')}\n"
        welcome_msg += f"- Total chunks: {stats.get('total_chunks', 0)}\n\n"
    
    welcome_msg += "## Comment Ã§a marche ?\n\n"
    welcome_msg += "1. **PremiÃ¨re gÃ©nÃ©ration** : Le backend appelle GPT/Claude et sauvegarde dans Redis (TTL 1h)\n"
    welcome_msg += "2. **GÃ©nÃ©rations suivantes** : Si mÃªme prompt dans l'heure, rÃ©ponse instantanÃ©e depuis Redis !\n"
    welcome_msg += "3. **Persistance** : Toutes les gÃ©nÃ©rations sont sauvegardÃ©es dans PostgreSQL\n\n"
    welcome_msg += "ğŸ’¡ **Astuce**: Envoyez le mÃªme message 2x pour voir la diffÃ©rence de vitesse !\n\n"
    welcome_msg += "ğŸ’¬ Envoyez un message pour dÃ©marrer !"
    
    await cl.Message(content=welcome_msg).send()

@cl.on_settings_update
async def update_settings(settings):
    """Mise Ã  jour des paramÃ¨tres"""
    
    provider_name = settings["Provider"]
    model = settings["Model"]
    temperature = settings["Temperature"]
    
    provider = "openai" if provider_name == "OpenAI GPT" else "anthropic"
    
    cl.user_session.set("provider", provider)
    cl.user_session.set("model", model)
    cl.user_session.set("temperature", temperature)
    
    await cl.Message(
        content=f"âœ… **Configuration mise Ã  jour**\n\n"
                f"- Provider: {provider_name}\n"
                f"- ModÃ¨le: {model}\n"
                f"- TempÃ©rature: {temperature}"
    ).send()

@cl.on_message
async def main(message: cl.Message):
    """Traitement du message"""
    
    # RÃ©cupÃ©rer config
    provider = cl.user_session.get("provider", "openai")
    model = cl.user_session.get("model", "gpt-4o")
    temperature = cl.user_session.get("temperature", 0.7)
    
    # Message de rÃ©ponse
    msg = cl.Message(content="")
    
    try:
        # Afficher info provider
        provider_name = "OpenAI GPT" if provider == "openai" else "Anthropic Claude"
        await msg.stream_token(f"ğŸ”„ **{provider_name}** - {model}\n")
        await msg.stream_token(f"ğŸ“‹ TempÃ©rature: {temperature}\n\n")
        await msg.stream_token("â³ CrÃ©ation de la tÃ¢che...\n\n")
        
        # CrÃ©er tÃ¢che sur backend
        task_data = await create_task(
            message.content,
            provider,
            model,
            temperature
        )
        
        task_id = task_data["task_id"]
        cached = task_data.get("cached", False)
        
        # Effacer "CrÃ©ation de la tÃ¢che..."
        msg.content = f"ğŸ”„ **{provider_name}** - {model}\n"
        msg.content += f"ğŸ“‹ TempÃ©rature: {temperature}\n"
        msg.content += f"ğŸ†” Task ID: `{task_id}`\n\n"
        msg.content += "---\n\n"
        await msg.update()
        
        # Stream depuis backend
        await stream_from_backend(task_id, msg, cached)
    
    except Exception as e:
        await msg.stream_token(f"\n\nâŒ **Erreur**: {str(e)}")
    
    # Envoyer
    await msg.send()
    
    # Metadata
    msg.metadata = {
        "provider": provider,
        "model": model,
        "temperature": temperature,
        "task_id": task_id,
        "cached": cached
    }
    await msg.update()

@cl.action_callback("refresh_stats")
async def refresh_stats(action):
    """RafraÃ®chir statistiques"""
    
    stats = await get_backend_stats()
    
    if stats:
        stats_msg = "## ğŸ“Š Statistiques Mises Ã  Jour\n\n"
        stats_msg += f"- Total gÃ©nÃ©rations: {stats.get('total_tasks', 0)}\n"
        stats_msg += f"- Depuis cache: {stats.get('cached_tasks', 0)}\n"
        stats_msg += f"- Taux de hit: {stats.get('cache_hit_rate', '0%')}\n"
        stats_msg += f"- Total chunks: {stats.get('total_chunks', 0)}\n"
        
        await cl.Message(content=stats_msg).send()
    else:
        await cl.Message(content="âŒ Impossible de rÃ©cupÃ©rer les statistiques").send()

# ==================== PROFILS DE CHAT ====================

@cl.set_chat_profiles
async def chat_profile():
    """DÃ©finir profils de chat"""
    
    # Check backend
    health = await check_backend_health()
    
    profiles = []
    
    if health.get("openai_configured"):
        profiles.append(
            cl.ChatProfile(
                name="GPT-4o-Cached",
                markdown_description="**OpenAI GPT-4o** avec cache Redis ultra-rapide",
                icon="https://cdn.openai.com/production/system-images/favicon-32x32.png"
            )
        )
    
    if health.get("anthropic_configured"):
        profiles.append(
            cl.ChatProfile(
                name="Claude-Cached",
                markdown_description="**Claude 3.5 Sonnet** avec cache Redis ultra-rapide",
                icon="https://www.anthropic.com/favicon.ico"
            )
        )
    
    if not profiles:
        profiles.append(
            cl.ChatProfile(
                name="Unconfigured",
                markdown_description="âš ï¸ Aucun provider configurÃ© sur le backend",
                icon="https://via.placeholder.com/32"
            )
        )
    
    return profiles
